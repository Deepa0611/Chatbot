{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chatbot Conversations From Customer Service Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys, os, re, itertools, collections, string, time\n",
    "from io import BytesIO\n",
    "from collections import Counter\n",
    "from time import time\n",
    "import datetime\n",
    "   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EKMRPRB\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3020: DtypeWarning: Columns (4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    }
   ],
   "source": [
    "# https://catalog.data.gov/dataset/consumer-complaint-database \n",
    "complaints_df_raw = pd.read_csv(\"consumer_complaints.csv\", \n",
    "                usecols=('Product','Consumer complaint narrative', 'Sub-issue'),\n",
    "                dtype={'consumer_complaint_narrative': object})\n",
    "# Only interested in data with consumer complaints\n",
    "complaints_df_raw=complaints_df_raw[complaints_df_raw['Consumer complaint narrative'].notnull()]\n",
    "complaints_df_raw=complaints_df_raw[complaints_df_raw['Product'].notnull()]\n",
    "\n",
    "# remove XXXX from narratives\n",
    "complaints_df_raw['Consumer complaint narrative'] =  complaints_df_raw['Consumer complaint narrative'].replace({'X':''}, regex=True)\n",
    "\n",
    "# always seed your random generators for reporducilibity \n",
    "complaints_df_raw = complaints_df_raw.sample(200000, replace=False, random_state=1)\n",
    "\n",
    "# basic sentence prep\n",
    "# set to lower\n",
    "complaints_df_raw['Consumer complaint narrative'] = complaints_df_raw['Consumer complaint narrative'].str.lower()\n",
    "# remove special characters\n",
    "complaints_df_raw['Consumer complaint narrative'] = complaints_df_raw['Consumer complaint narrative'].str.replace('\\W', ' ')\n",
    "\n",
    "# remove elements with no text\n",
    "complaints_df_raw= complaints_df_raw[complaints_df_raw['Consumer complaint narrative'] != '']\n",
    "\n",
    "# any dups\n",
    "complaints_df_raw = complaints_df_raw.drop_duplicates(subset=['Consumer complaint narrative'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Product</th>\n",
       "      <th>Sub-issue</th>\n",
       "      <th>Consumer complaint narrative</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>377311</th>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Old information reappears or never goes away</td>\n",
       "      <td>fl    this account was over    and c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156975</th>\n",
       "      <td>Debt collection</td>\n",
       "      <td>Debt is not yours</td>\n",
       "      <td>on xx xx xxxx a friend family member was searc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>732081</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>this is a transfer of servicing issue  my mort...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438555</th>\n",
       "      <td>Mortgage</td>\n",
       "      <td>NaN</td>\n",
       "      <td>i knew we were behind  and i  ve been working ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>430892</th>\n",
       "      <td>Credit reporting, credit repair services, or o...</td>\n",
       "      <td>Their investigation did not fix an error on yo...</td>\n",
       "      <td>i have a collection account on my three   3   ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Product  \\\n",
       "377311  Credit reporting, credit repair services, or o...   \n",
       "156975                                    Debt collection   \n",
       "732081                                           Mortgage   \n",
       "438555                                           Mortgage   \n",
       "430892  Credit reporting, credit repair services, or o...   \n",
       "\n",
       "                                                Sub-issue  \\\n",
       "377311       Old information reappears or never goes away   \n",
       "156975                                  Debt is not yours   \n",
       "732081                                                NaN   \n",
       "438555                                                NaN   \n",
       "430892  Their investigation did not fix an error on yo...   \n",
       "\n",
       "                             Consumer complaint narrative  \n",
       "377311            fl    this account was over    and c...  \n",
       "156975  on xx xx xxxx a friend family member was searc...  \n",
       "732081  this is a transfer of servicing issue  my mort...  \n",
       "438555  i knew we were behind  and i  ve been working ...  \n",
       "430892  i have a collection account on my three   3   ...  "
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complaints_df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean Up Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "complaints_df = complaints_df_raw.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(194141, 3)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_similarity=complaints_df['Consumer complaint narrative'].str.split(' ').map(Counter)\n",
    "word_similarity_ratio = []\n",
    "complaints_df.shape "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    194141.000000\n",
       "mean          2.101858\n",
       "std           1.096282\n",
       "min           1.000000\n",
       "25%           1.620000\n",
       "50%           1.969231\n",
       "75%           2.409836\n",
       "max         229.681818\n",
       "Name: narrative_similarity_ratio, dtype: float64"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for wu in word_similarity:\n",
    "    word_similarity_ratio.append(np.sum([x[1] for x in wu.items()])/np.float(len(wu)))\n",
    "    \n",
    "complaints_df['narrative_similarity_ratio'] = word_similarity_ratio\n",
    "complaints_df['narrative_similarity_ratio'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(59635, 4)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# thin out some entries that contain too much duplicated lines within\n",
    "complaints_df = complaints_df[complaints_df['narrative_similarity_ratio'] <= 1.7]\n",
    "complaints_df.reset_index(drop=True,inplace=True)\n",
    "complaints_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i never authorize this account to be open at no point and i have call the bank to fix this and they only give the run round everything  i contacted them 3 days ago and they said i dont have no account with a negative balance but in my xxxx xxxx it came up with a negative balance and this is not letting me open other banks account ',\n",
       " 'deposited my personal checks in my account  chase shut my account and stole the xxxx xxxx dollars    after the funds cleared the account they shut my account and unwilling to return my funds claiming that they can not verify im working for this employee    ',\n",
       " 'i suspect a scam to prolong my pmi payments  i was advised that an appraiser will contact me in a month  it took over 6 months to get an appraiser scheduled  then once completed  with a 25   debt to value ratio  i was denied  it is now being reviewed  and it will take another 2 weeks for the results  culprit loancare   escrow  xxxx  va ',\n",
       " 'getting repeated harrassing phone calls stating there are two allogations against my social security number for unpaid debt and that they are sending a process server to my home  which never actually happens of course  but the calls are getting annoying ']"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(complaints_df['Consumer complaint narrative'])[0:4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get Key Verbs And Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find most common verbs and measure coverage \n",
    "import spacy\n",
    "# Load English tokenizer, tagger, parser, NER and word vectors\n",
    "#nlp = spacy.load('en')\n",
    "\n",
    "# just load what we need to avoid taxing memory\n",
    "nlp = spacy.load('en', parser=False, entity=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "22"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create one big blob of text to process things a bit faster\n",
    "blob_complaints = ''.join(list(complaints_df['Consumer complaint narrative']))\n",
    "\n",
    "# Max text of length of 1000000\n",
    "n = 900000\n",
    "blog_chunks = [blob_complaints[i:i+n] for i in range(0, len(blob_complaints), n)]\n",
    "len(blog_chunks)\n",
    "#blog_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21\n",
      "20\n",
      "20\n",
      "19\n",
      "18\n",
      "17\n",
      "16\n",
      "15\n",
      "14\n",
      "13\n",
      "12\n",
      "11\n",
      "10\n",
      "10\n",
      "9\n",
      "8\n",
      "7\n",
      "6\n",
      "5\n",
      "4\n",
      "3\n",
      "2\n",
      "1\n",
      "0\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "just_verbs = []\n",
    "just_nouns = []\n",
    "counter_=len(blog_chunks)\n",
    "for sentence in blog_chunks:\n",
    "    counter_ -= 1\n",
    "    if (counter_ % 10 == 0): print(counter_)\n",
    "    print(counter_)\n",
    "    doc = nlp(sentence.encode().decode('utf-8'))\n",
    "    temp_verb = []\n",
    "    temp_noun = []\n",
    "    for token in doc: \n",
    "        if (token.pos_ == u'VERB'): \n",
    "            temp_verb.append(token.text)\n",
    "        if (token.pos_ == u'NOUN'):\n",
    "            temp_noun.append(token.text)\n",
    "            \n",
    "\n",
    "    just_verbs.append(' '.join(temp_verb).encode('utf-8'))\n",
    "    just_nouns.append(' '.join(temp_noun).encode('utf-8'))\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'authorize',\n",
       " b'be',\n",
       " b'have',\n",
       " b'call',\n",
       " b'fix',\n",
       " b'give',\n",
       " b'contacted',\n",
       " b'said',\n",
       " b'do',\n",
       " b'have']"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "just_verbs[0].split()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[b'account',\n",
       " b'point',\n",
       " b'bank',\n",
       " b'run',\n",
       " b'everything',\n",
       " b'days',\n",
       " b'account',\n",
       " b'balance',\n",
       " b'xxxx',\n",
       " b'balance']"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "just_nouns[0].split()[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count just_verbs: 22\n",
      "count just_nouns: 22\n"
     ]
    }
   ],
   "source": [
    "print('count just_verbs: %i' % len(just_verbs))\n",
    "print('count just_nouns: %i' % len(just_nouns))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle both objects so you don't have to re-run spacy \n",
    "import pickle\n",
    "pickle_file = \"verbs_nouns.p\"\n",
    "\n",
    "overwrite_old_pickle = True\n",
    "if overwrite_old_pickle:\n",
    "    with open(pickle_file, \"wb\") as f:\n",
    "        pickle.dump([just_verbs, just_nouns], f)\n",
    "    \n",
    "# read in saved pickle\n",
    "with open(pickle_file, \"rb\") as f:\n",
    "    backup_pos = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Out Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "836837"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_verbs = backup_pos[0]\n",
    "len(all_verbs)\n",
    "\n",
    "# append all verbs together so we can run frequency counts\n",
    "verbs = []\n",
    "for verb_set in all_verbs:\n",
    "    verbs.append(verb_set.split())\n",
    "    #verbs = [verb for verb in verb_set[0].split()]\n",
    "\n",
    "len(verbs)\n",
    "verbs_master = [val for sublist in verbs for val in sublist]\n",
    "len(verbs_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>verb</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'have'</td>\n",
       "      <td>52734</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'is'</td>\n",
       "      <td>42147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'was'</td>\n",
       "      <td>39680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'xxxx'</td>\n",
       "      <td>24119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'has'</td>\n",
       "      <td>17523</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>b'are'</td>\n",
       "      <td>16991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>b'been'</td>\n",
       "      <td>16264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>b'be'</td>\n",
       "      <td>15060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>b'do'</td>\n",
       "      <td>12603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>b'had'</td>\n",
       "      <td>11272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>b'am'</td>\n",
       "      <td>11254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>b'did'</td>\n",
       "      <td>9817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>b'received'</td>\n",
       "      <td>8565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>b'sent'</td>\n",
       "      <td>8164</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>b'would'</td>\n",
       "      <td>8154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>b'paid'</td>\n",
       "      <td>7435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>b'were'</td>\n",
       "      <td>7329</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>b'get'</td>\n",
       "      <td>7290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>b'called'</td>\n",
       "      <td>7242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>b'can'</td>\n",
       "      <td>6940</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           verb  count\n",
       "0       b'have'  52734\n",
       "1         b'is'  42147\n",
       "2        b'was'  39680\n",
       "3       b'xxxx'  24119\n",
       "4        b'has'  17523\n",
       "5        b'are'  16991\n",
       "6       b'been'  16264\n",
       "7         b'be'  15060\n",
       "8         b'do'  12603\n",
       "9        b'had'  11272\n",
       "10        b'am'  11254\n",
       "11       b'did'   9817\n",
       "12  b'received'   8565\n",
       "13      b'sent'   8164\n",
       "14     b'would'   8154\n",
       "15      b'paid'   7435\n",
       "16      b'were'   7329\n",
       "17       b'get'   7290\n",
       "18    b'called'   7242\n",
       "19       b'can'   6940"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is your upper and lower cut offs?\n",
    "from collections import Counter\n",
    "verbs_df = pd.DataFrame(Counter([verb for verb in verbs_master]).most_common(), columns = ['verb', 'count'])\n",
    "verbs_df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "135"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(verbs_df[verbs_df['count'] > 1000])\n",
    "verbs_df = verbs_df[verbs_df['count'] > 1000]\n",
    "len(verbs_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sorting Out Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "881007"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_nouns = backup_pos[1]\n",
    "\n",
    "# append all verbs together so we can run frequency counts\n",
    "nouns = []\n",
    "for noun_set in all_nouns:\n",
    "    nouns.append(noun_set.split())\n",
    "\n",
    "nouns_master = [val for sublist in nouns for val in sublist]\n",
    "len(nouns_master)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>noun</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>b'xxxx'</td>\n",
       "      <td>77833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>b'credit'</td>\n",
       "      <td>45882</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>b'account'</td>\n",
       "      <td>26849</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b'report'</td>\n",
       "      <td>20325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>b'debt'</td>\n",
       "      <td>17253</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         noun  count\n",
       "0     b'xxxx'  77833\n",
       "1   b'credit'  45882\n",
       "2  b'account'  26849\n",
       "3   b'report'  20325\n",
       "4     b'debt'  17253"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what is your upper and lower cut offs?\n",
    "from collections import Counter\n",
    "nouns_df = pd.DataFrame(Counter([noun for noun in nouns_master]).most_common(), columns = ['noun', 'count'])\n",
    "nouns_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "157"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(nouns_df[nouns_df['count'] > 1000])\n",
    "nouns_df = nouns_df[nouns_df['count'] > 1000]\n",
    "len(nouns_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binarize DataFrame With Official Verb & Noun List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50000\n",
      "40000\n",
      "30000\n",
      "20000\n",
      "10000\n",
      "0\n",
      "length: 59635\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(59635, 292)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create new data frame with key verbs and nouns as features\n",
    "key_words = list(nouns_df['noun']) + list(verbs_df['verb'])\n",
    "row_bools = []\n",
    "counter_ = len(complaints_df['Consumer complaint narrative'])\n",
    "for sentence in complaints_df['Consumer complaint narrative']:\n",
    "    counter_ -= 1\n",
    "    if (counter_ % 10000 == 0): print(counter_)\n",
    "    row_bool = []\n",
    "    words = sentence.split()\n",
    "    for kw in key_words:\n",
    "        row_bool.append(kw in words)\n",
    "    row_bools.append(row_bool)\n",
    "    \n",
    "print('length:', len(row_bools))\n",
    "row_bools = pd.DataFrame(row_bools, columns=key_words)    \n",
    "row_bools = row_bools.astype(int)\n",
    "row_bools.shape\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>b'xxxx'</th>\n",
       "      <th>b'credit'</th>\n",
       "      <th>b'account'</th>\n",
       "      <th>b'report'</th>\n",
       "      <th>b'debt'</th>\n",
       "      <th>b'information'</th>\n",
       "      <th>b'company'</th>\n",
       "      <th>b'loan'</th>\n",
       "      <th>b'payment'</th>\n",
       "      <th>b'bank'</th>\n",
       "      <th>...</th>\n",
       "      <th>b'feel'</th>\n",
       "      <th>b'spoke'</th>\n",
       "      <th>b'work'</th>\n",
       "      <th>b're'</th>\n",
       "      <th>b'delete'</th>\n",
       "      <th>b'gave'</th>\n",
       "      <th>b'ca'</th>\n",
       "      <th>b'noticed'</th>\n",
       "      <th>b'must'</th>\n",
       "      <th>b'responded'</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 292 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   b'xxxx'  b'credit'  b'account'  b'report'  b'debt'  b'information'  \\\n",
       "0        0          0           0          0        0               0   \n",
       "1        0          0           0          0        0               0   \n",
       "2        0          0           0          0        0               0   \n",
       "3        0          0           0          0        0               0   \n",
       "4        0          0           0          0        0               0   \n",
       "\n",
       "   b'company'  b'loan'  b'payment'  b'bank'  ...  b'feel'  b'spoke'  b'work'  \\\n",
       "0           0        0           0        0  ...        0         0        0   \n",
       "1           0        0           0        0  ...        0         0        0   \n",
       "2           0        0           0        0  ...        0         0        0   \n",
       "3           0        0           0        0  ...        0         0        0   \n",
       "4           0        0           0        0  ...        0         0        0   \n",
       "\n",
       "   b're'  b'delete'  b'gave'  b'ca'  b'noticed'  b'must'  b'responded'  \n",
       "0      0          0        0      0           0        0             0  \n",
       "1      0          0        0      0           0        0             0  \n",
       "2      0          0        0      0           0        0             0  \n",
       "3      0          0        0      0           0        0             0  \n",
       "4      0          0        0      0           0        0             0  \n",
       "\n",
       "[5 rows x 292 columns]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_bools.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster of popular sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\EKMRPRB\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\cluster\\k_means_.py:971: ConvergenceWarning: Number of distinct clusters (1) found smaller than n_clusters (50). Possibly due to duplicate points in X.\n",
      "  return_n_iter=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    59635\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "TOTAL_CLUSTERS = 50\n",
    "\n",
    "# Number of clusters\n",
    "kmeans = KMeans(n_clusters=TOTAL_CLUSTERS)\n",
    "# Fitting the input data\n",
    "kmeans = kmeans.fit(row_bools)\n",
    "# Getting the cluster labels\n",
    "labels = kmeans.predict(row_bools)\n",
    "\n",
    "# add cluster back to data frame \n",
    "row_bools['cluster'] = labels\n",
    "\n",
    "row_bools['cluster'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    59635\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "row_bools['cluster'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster: 0\n",
      "data cluster shape: 59635\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# add cluster number back to orginal corpus\n",
    "complaints_df['Cluster'] = labels\n",
    "# import sys\n",
    "# reload(sys)\n",
    "# sys.setdefaultencoding('utf8')\n",
    "import itertools\n",
    "from collections import Counter\n",
    "import nltk\n",
    "from nltk.util import ngrams\n",
    "\n",
    "unique_complaints_2grams = []\n",
    "unique_complaints_3grams = []\n",
    "unique_complaints_4grams = []\n",
    "unique_complaints_5grams = []\n",
    "unique_complaints_6grams = []\n",
    "# loop through each cluster\n",
    "for cluster_to_search in range(min(row_bools['cluster']), max(row_bools['cluster'])+1):\n",
    "    # cluster-level research\n",
    "    print('Cluster: %i' % cluster_to_search)\n",
    "    df_tmp = complaints_df[complaints_df['Cluster']==cluster_to_search].copy()\n",
    "    print('data cluster shape: %s' % len(df_tmp))\n",
    "    \n",
    "    bigrams = []\n",
    "    trigrams = []\n",
    "    fourgrams = []\n",
    "    fivegrams = []\n",
    "    sixgrams = []\n",
    "    \n",
    "    for index, row in df_tmp.iterrows(): \n",
    "        token = nltk.word_tokenize(row['Consumer complaint narrative'].encode().decode('utf-8'))\n",
    "        bigrams.append([' '.join(pair) for pair in list(ngrams(token,2)) if len(set(pair))==2])\n",
    "        trigrams.append([' '.join(pair) for pair in list(ngrams(token,3)) if len(set(pair))==3])\n",
    "        fourgrams.append([' '.join(pair) for pair in list(ngrams(token,4)) if len(set(pair))==4])\n",
    "        fivegrams.append([' '.join(pair) for pair in list(ngrams(token,5)) if len(set(pair))==5])\n",
    "        sixgrams.append([' '.join(pair) for pair in list(ngrams(token,6)) if len(set(pair))==6])\n",
    "        \n",
    "    bigrams = [val for sublist in bigrams for val in sublist]\n",
    "    trigrams = [val for sublist in trigrams for val in sublist]\n",
    "    fourgrams = [val for sublist in fourgrams for val in sublist]\n",
    "    fivegrams = [val for sublist in fivegrams for val in sublist]\n",
    "    sixgrams = [val for sublist in sixgrams for val in sublist]\n",
    "    \n",
    "    # find top x most popular grams per size\n",
    "    # 2 bigrams\n",
    "    freqx = pd.DataFrame(Counter([noun for noun in bigrams]).most_common(50), columns=['bigrams','frequency'])\n",
    "    freqx['Cluster'] = cluster_to_search\n",
    "    unique_complaints_2grams.append(freqx)\n",
    "    # 3 bigrams\n",
    "    freqx = pd.DataFrame(Counter([noun for noun in trigrams]).most_common(50), columns=['trigrams','frequency'])\n",
    "    freqx['Cluster'] = cluster_to_search\n",
    "    unique_complaints_3grams.append(freqx)\n",
    "    # 4 bigrams\n",
    "    freqx = pd.DataFrame(Counter([noun for noun in fourgrams]).most_common(50), columns=['fourgrams','frequency'])\n",
    "    freqx['Cluster'] = cluster_to_search\n",
    "    unique_complaints_4grams.append(freqx)\n",
    "    # 5 bigrams\n",
    "    freqx = pd.DataFrame(Counter([noun for noun in fivegrams]).most_common(50), columns=['fivegrams','frequency'])\n",
    "    freqx['Cluster'] = cluster_to_search\n",
    "    unique_complaints_5grams.append(freqx)\n",
    "    # 6 bigrams\n",
    "    freqx = pd.DataFrame(Counter([noun for noun in sixgrams]).most_common(50), columns=['sixgrams','frequency'])\n",
    "    freqx['Cluster'] = cluster_to_search\n",
    "    unique_complaints_6grams.append(freqx)\n",
    " \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>fourgrams</th>\n",
       "      <th>frequency</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>on my credit report</td>\n",
       "      <td>6051</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>from my credit report</td>\n",
       "      <td>1709</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>my credit report and</td>\n",
       "      <td>1465</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>i do n t</td>\n",
       "      <td>1361</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>my credit report i</td>\n",
       "      <td>1315</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               fourgrams  frequency  Cluster\n",
       "0    on my credit report       6051        0\n",
       "1  from my credit report       1709        0\n",
       "2   my credit report and       1465        0\n",
       "3               i do n t       1361        0\n",
       "4     my credit report i       1315        0"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "    df = pd.concat(unique_complaints_4grams)\n",
    "    # freqx = pd.DataFrame(Counter([noun for noun in fourgrams]).most_common(50), columns=['fourgrams','frequency'])\n",
    "    df = df.drop_duplicates(subset=['fourgrams'], keep=False)\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sixgrams</th>\n",
       "      <th>frequency</th>\n",
       "      <th>Cluster</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am a victim of identity</td>\n",
       "      <td>484</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>when i reviewed my credit report</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>promptly delete all information which can</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>information which can not be verified</td>\n",
       "      <td>176</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>when i called the company they</td>\n",
       "      <td>175</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>has been non compliant with removing</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>been non compliant with removing the</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>non compliant with removing the unverified</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>611 5 a of the fcra</td>\n",
       "      <td>174</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>compliant with removing the unverified account</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>also when i called the company</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>i called the company they responded</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>called the company they responded that</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>did not send them all of</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>not send them all of the</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>send them all of the verifying</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>them all of the verifying evidence</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>all of the verifying evidence to</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>of the verifying evidence to confirm</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>the verifying evidence to confirm each</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>verifying evidence to confirm each account</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>evidence to confirm each account is</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>section 611 5 a of the</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>was a victim of identity theft</td>\n",
       "      <td>177</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>sure how this happened i believe</td>\n",
       "      <td>179</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>delete all information which can not</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>am a victim of identity theft</td>\n",
       "      <td>480</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>a victim of identity theft and</td>\n",
       "      <td>379</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>on my credit report i have</td>\n",
       "      <td>263</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>that do not belong to me</td>\n",
       "      <td>232</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>be removed from my credit report</td>\n",
       "      <td>228</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>not sure how this happened i</td>\n",
       "      <td>208</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>i am not sure how this</td>\n",
       "      <td>206</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>statements my only thought is that</td>\n",
       "      <td>203</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>that does not belong to me</td>\n",
       "      <td>202</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>i have no knowledge of this</td>\n",
       "      <td>192</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>to confirm each account is unverified</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>am not sure how this happened</td>\n",
       "      <td>190</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>to you when i received my</td>\n",
       "      <td>187</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>statement did not get to me</td>\n",
       "      <td>185</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>my payments to you when i</td>\n",
       "      <td>184</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>on my credit report that i</td>\n",
       "      <td>183</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>payments to you when i received</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>i was a victim of identity</td>\n",
       "      <td>182</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>reviewed my credit report and found</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>made my payments to you when</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>you when i received my statements</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>all information which can not be</td>\n",
       "      <td>181</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>i reviewed my credit report and</td>\n",
       "      <td>189</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>of the fair credit reporting act</td>\n",
       "      <td>173</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          sixgrams  frequency  Cluster\n",
       "0                        i am a victim of identity        484        0\n",
       "27                when i reviewed my credit report        176        0\n",
       "28       promptly delete all information which can        176        0\n",
       "29           information which can not be verified        176        0\n",
       "30                  when i called the company they        175        0\n",
       "31            has been non compliant with removing        174        0\n",
       "32            been non compliant with removing the        174        0\n",
       "33      non compliant with removing the unverified        174        0\n",
       "34                             611 5 a of the fcra        174        0\n",
       "35  compliant with removing the unverified account        173        0\n",
       "36                  also when i called the company        173        0\n",
       "37             i called the company they responded        173        0\n",
       "38          called the company they responded that        173        0\n",
       "39                        did not send them all of        173        0\n",
       "40                        not send them all of the        173        0\n",
       "41                  send them all of the verifying        173        0\n",
       "42              them all of the verifying evidence        173        0\n",
       "43                all of the verifying evidence to        173        0\n",
       "44            of the verifying evidence to confirm        173        0\n",
       "45          the verifying evidence to confirm each        173        0\n",
       "46      verifying evidence to confirm each account        173        0\n",
       "47             evidence to confirm each account is        173        0\n",
       "26                          section 611 5 a of the        177        0\n",
       "25                  was a victim of identity theft        177        0\n",
       "24                sure how this happened i believe        179        0\n",
       "23            delete all information which can not        180        0\n",
       "1                    am a victim of identity theft        480        0\n",
       "2                   a victim of identity theft and        379        0\n",
       "3                       on my credit report i have        263        0\n",
       "4                         that do not belong to me        232        0\n",
       "5                 be removed from my credit report        228        0\n",
       "6                     not sure how this happened i        208        0\n",
       "7                           i am not sure how this        206        0\n",
       "8               statements my only thought is that        203        0\n",
       "9                       that does not belong to me        202        0\n",
       "10                     i have no knowledge of this        192        0\n",
       "48           to confirm each account is unverified        173        0\n",
       "11                   am not sure how this happened        190        0\n",
       "13                       to you when i received my        187        0\n",
       "14                     statement did not get to me        185        0\n",
       "15                       my payments to you when i        184        0\n",
       "16                      on my credit report that i        183        0\n",
       "17                 payments to you when i received        182        0\n",
       "18                      i was a victim of identity        182        0\n",
       "19             reviewed my credit report and found        181        0\n",
       "20                    made my payments to you when        181        0\n",
       "21               you when i received my statements        181        0\n",
       "22                all information which can not be        181        0\n",
       "12                 i reviewed my credit report and        189        0\n",
       "49                of the fair credit reporting act        173        0"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# find top x most popular grams per size\n",
    "see_grams = 6\n",
    "\n",
    "\n",
    "if see_grams==2:\n",
    "    df = pd.concat(unique_complaints_2grams)\n",
    "    df = df.drop_duplicates(subset=['bigrams'], keep=False)\n",
    "elif see_grams==3:\n",
    "    df = pd.concat(unique_complaints_3grams)\n",
    "    df = df.drop_duplicates(subset=['trigrams'], keep=False)\n",
    "elif see_grams==4:\n",
    "    df = pd.concat(unique_complaints_4grams)\n",
    "    df = df.drop_duplicates(subset=['fourgrams'], keep=False)\n",
    "elif see_grams==5:\n",
    "    df = pd.concat(unique_complaints_5grams)\n",
    "    df = df.drop_duplicates(subset=['fivegrams'], keep=False)\n",
    "elif see_grams==6:\n",
    "    df = pd.concat(unique_complaints_6grams)\n",
    "    df = df.drop_duplicates(subset=['sixgrams'], keep=False)\n",
    " \n",
    "df = df.sort_values('Cluster')\n",
    "df[df['frequency'] > 10]  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tie It Back To Complaint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this company is attempting to collect a debt from me that is not mine \n",
      "------\n",
      "portfolio recovery associates of xxxx  va  has made numerous phone calls to my home attempting to collect a debt from a third party after confirmed receipt of a certified letter from me telling them the party they are attempting to contact does not live at my address  does not have my phone number  and not to contact me any more regarding this matter \n",
      "------\n",
      "i have received a xxxx xxxx 2015 letter and various phone calls from central credit services  llc attempting to collect a debt from xxxx in xxxx  these are most likely about a xxxx services accrued in a bundled service  i have statements from xxxx showing that the statements are paid in full  i previously filed a complaint with the ftc regarding this manner and i thought it was taken care of  now i am receiving bills at my new location in xxxx \n",
      "------\n",
      "i had forster garbus  llp contact me at my place of employment  which my employer does not allow  from phone number xxxx  they told me they are attempting to collect a debt from xxxx  a company i have never heard of  this debt they claim i have never been contacted about in the past  it does not show up on my credit report  and i have never heard of it  it is fraudulent and so was the harassing phone call \n",
      "------\n",
      "ras lavrar is attempting to collect a debt from me that i have no recollection of  ras lavrar has stepped outside the boundaries fdpa  i have received a lot of harassment calls and even a representative came to my home or residences with harassments that are in violation of the fdpa  i never established a contract with this company and i don t owe the alleged debt i will be reporting this practice to the state attorney general office of ga and the ftc about this matter \n",
      "------\n",
      " is attempting to collect a debt from me that i never establish a contract with this company nor do i owe any money to this company they are violating the fcra and placed negative reporting on both my  and  reports for amounts of  and   this debt is not collectable  these student loans were consolidated and i was never late \n",
      "------\n",
      "i am not the one the company is attempting to collect a debt from  i continuously receive automated phone calls that do not allow me the option to stop contact  these calls happen at least twice a week  usually always on a monday and sometimes on either thursdays or fridays   happening since i received this new phone number back in xxxx 2016  the person these companies are trying to collect a debt from is not me as the name they have stated on the recordings voicemails  i have added my number to the do not call registry but that has not affected these calls \n",
      "------\n",
      "stellar recovery inc is attempting to collect a debt from me that i do n t owe them  i never established a contract with this company and they have reported negative items onto my xxxx and xxxx credit reports which is in violation of the fcra \n",
      "------\n",
      "national credit system is attempting to collect a debt from me for xxxx dollars and i never established a contract with this company furthermore this company has reported negative items on to my credit report and is violating the fcra standard \n",
      "------\n",
      "i am a victim of identity theft  portfolio recovery is attempting to collect a debt that was already flagged by the original creditor as identity theft  portfolio recovery is attempting to collect a debt from xxxx xxxx that was not opened by me \n",
      "------\n",
      "wakefield and associates of xxxx  colorado  has been attempting to collect a debt from me for one year  i have never received proof of the debt in writing they have called me multiple times during the day and evening  they have argued with me and have said they do not need to send paper proof of debts  the collection reported on my credit report does not match the amount they are seeking \n",
      "------\n",
      "i have received mutilate calls from them attempting to collect a debt from a medical visit i never had  if i had visited  my health insurname would have covered the visit and it would n t have cost me   260 00  like they are claiming \n",
      "------\n",
      "diversified has called my office on multiple occasions from different numbers attempting to collect a debt from someone not associated with this office or myself    xxxx xxxx we  ve asked them to stop calling but they continue and i feel as if they are violating the fdcpa    can you please assist \n",
      "------\n",
      "the collection agency that is attempting to collect a debt from me is not licensed to operate in my state  thus  i am disputing this collection  i have included the list of all active licenses for collection agencies from my state  s government website \n",
      "------\n",
      "rash curtis and associates is attempting to collect a debt from me that i have no awareness  via threatening to damage my credit \n",
      "------\n",
      "the collection agency ic systems is attempting to collect a debt from me on behalf of xxxx for unreturned cable equipment  firstly  xxxx has confirmed that i did return the equipment and owe them nothing  additionally  xxxx has told me that ic system is no longer an authorized collecter for xxxx  and is in fact harassing xxxx customers \n",
      "------\n",
      "maury cobb  attorney at law  located in xxxx  al keeps sending me robotic calls from xxxx asking me to call xxxx  daily robotic calls to my cell phone attempting to collect a debt from someone who is unknown to me  i have contacted the caller twice to notify of the problem  but they have not changed tactics or otherwise changed their calling pattern  by nature of their systematic inability to source or acknowledge the problem i am effectively being harassed and am seeking relief \n",
      "------\n",
      "accouny discovery systems disclosed information about my debt to a third party without my consent  today after requesting them not to contact me at work or any other person who is not me trying to get a hold of me  they dishonored my request and contacted my mother at her job  instead of keeping my information discreet  they shared that they were attempting to collect a debt from me after my mother asked who was calling  is n t this supposed to be against the law  the fact that i am in debt should not be shared with anybody without my consent  i am getting tired of them harassing me and calling my employer and family members several times a day  i feel helpless \n",
      "------\n",
      "i am dealing with the collection agency synergetic communication   inc  i am questioning how legitimate this company is for several reasons  they do not show up as the creditor on my credit report  and they are unlawfully attempting to collect a debt from me by violating several fdcpa laws  they have contacted me before xxxx my time zone and have also called me several times within one hour  they have also never sent me a debt validation letter to make me aware they are the collection agency assigned and to advise me of my rights to dispute the validity of the debt \n",
      "------\n",
      "this company northwood asset management has repeatedly contacted me and other 3rd party persons not associated with the person they attempting to collect a debt from  when asked to stop and told they can not call us they have threatened and used obscene language and threatened us  when told this is not the number to contact said debtor they insist we must give them information about us and the person they are attempting to contact \n",
      "------\n",
      "united collection bureau   robo calls    my home phone number daily attempting to collect a debt from some one else whose former phone number i was likely assigned after a recent move  i have called them back several times and have left messages for them to stop these calls as i am a new person with the debtor  s former number  i even spoke to a gentleman there once and explained to him that the number they have on file is no longer for the person which they are still attempting to reach  their calls have continued despite my threatening them with legal action \n",
      "------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  is attempting to collect a debt from a company i know nothing about  she threatened me and stated   she was going to contact my employer and there was a warrant out for my arrest from check enforcement  i asked for her to submit a document to show i owe this debt but was unsuccessful  she calling from   not sure what to do becuase she harassing me at work \n",
      "------\n",
      "i received a message from global credit and collection company who said it was attempting to collect a debt i owe  i called the number back who said it was a non working number for xxxx  this same company left a message on xxxx other phone numbers that have never been associated with any account i have  it was disclosed on answering machines voicemail that they were attempting to collect a debt from me \n",
      "------\n",
      "collection services of athens is attempting to collect a debt from me and i never established a contract with this company  furthermore collection services of athens is currently reporting negative information onto my xxxx credit report indicating that i owe xxxx amount of xxxx and a additional amount for xxxx dollars \n",
      "------\n",
      "i recieved calls froma company called cbe group attempting to collect a debt from xxxx xxxx  after reviewing my credit report this appears to have been removed and is showing back up on my credit reports  i asked that they no longer call my phone attempting to collect a debt that is not on my credit report \n",
      "------\n"
     ]
    }
   ],
   "source": [
    "# tie it back to look into a couple of actual complaints\n",
    "keywords = \"attempting to collect a debt from\"\n",
    " \n",
    "for index, row in complaints_df.iterrows():\n",
    "    txt = row['Consumer complaint narrative'] \n",
    "    if (keywords in txt):\n",
    "        print(txt)\n",
    "        print('------')\n",
    "    \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
